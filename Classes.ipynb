{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asiye\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report,mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from statsmodels.stats.diagnostic import normal_ad\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "#from lightgbm import LGBMClassifier\n",
    "#from catboost import CatBoostClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "from termcolor import colored\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Information:\n",
    "    \n",
    "    def __init__(self,data):\n",
    "        self.data = data;\n",
    "        \n",
    "    def data_features(self):\n",
    "        print(\"--------------------------  DATA HEAD --------------------------\\n\")\n",
    "        print(self.data.head())\n",
    "        print()\n",
    "        \n",
    "        print(\"--------------------------  DATA DESCRIBE --------------------------\\n\")\n",
    "        print(self.data.describe().T)\n",
    "        print()\n",
    "\n",
    "        print(\"--------------------------  DATA INFO --------------------------\\n\")\n",
    "        print(self.data.info())\n",
    "        print()\n",
    "     \n",
    "        print(\"--------------------------  DATA SHAPE --------------------------\\n\")\n",
    "        print(self.data.shape)\n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    def __init__(self):\n",
    "        print()\n",
    "    \n",
    "    def barplot(self,column1,column2):\n",
    "        pd.crosstab(column1,column2).plot(kind='bar')\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    def missing_values(self,data):\n",
    "        self.data = data\n",
    "        plt.subplots(figsize=(12, 6))\n",
    "        plt.title('Missing Values')\n",
    "        sns.heatmap(self.data.isnull(), yticklabels = False, cmap=\"viridis\")\n",
    "        plt.show()\n",
    "        \n",
    "    def confusion_matris(self,y,y_pred):\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.imshow(cm)\n",
    "        ax.grid(False)\n",
    "        ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "        ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "        ax.set_ylim(1.5, -0.5)\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "        plt.show()\n",
    "        \n",
    "    def corrMatrix(self,data):\n",
    "        corr = data.corr()\n",
    "        plt.figure(figsize=(20,20))\n",
    "        \n",
    "        ax = sns.heatmap(\n",
    "            corr, \n",
    "            vmin=-1, vmax=1, center=0,\n",
    "            cmap=sns.diverging_palette(20, 220, n=200),\n",
    "            square=True\n",
    "        )\n",
    "        ax.set_xticklabels(\n",
    "            ax.get_xticklabels(),\n",
    "            rotation=45,\n",
    "            horizontalalignment='right'\n",
    "        );\n",
    "        \n",
    "    def Elbow(self,range_nums,data):\n",
    "        wcss = []\n",
    " \n",
    "        for k in range_nums: \n",
    "            #Building and fitting the model \n",
    "            kmeanModel = KMeans(n_clusters=k)\n",
    "            kmeanModel.fit(data)     \n",
    "\n",
    "\n",
    "            wcss.append(kmeanModel.inertia_) \n",
    "\n",
    "        plt.plot(range_nums,wcss)\n",
    "        plt.xlabel(\"Number of k (cluster) value\")\n",
    "        plt.ylabel(\"wcss\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def GraphTree(self,model,X_train):  # Çalışmıyooor!!\n",
    "        \n",
    "        feature_cols = X_train.columns\n",
    "        \n",
    "        dot_data = StringIO()\n",
    "        export_graphviz(model, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\n",
    "        graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "        #graph.write_png('DecisionTreeCls.png')\n",
    "        Image(graph.create_png())\n",
    "        \n",
    "    def ROCCurve(self,X_test,y_test,y_pred,model):\n",
    "        \n",
    "        logit_roc_auc = roc_auc_score(y_test,y_pred)\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)\n",
    "        plt.plot([0, 1], [0, 1],'r--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Oranı')\n",
    "        plt.ylabel('True Positive Oranı')\n",
    "        plt.title('ROC')\n",
    "        plt.show()\n",
    "        \n",
    "    # Linear Reg Assumptions\n",
    "\n",
    "    def calculate_residuals(model, features, label):\n",
    "        \"\"\"\n",
    "        Creates predictions on the features with the model and calculates residuals\n",
    "        \"\"\"\n",
    "        predictions = model.predict(features)\n",
    "        df_results = pd.DataFrame({'Actual': label, 'Predicted': predictions})\n",
    "        df_results['Residuals'] = abs(df_results['Actual']) - abs(df_results['Predicted'])\n",
    "\n",
    "        return df_results    \n",
    "    \n",
    "    def normal_errors_assumption(self,model, features, label, p_value_thresh=0.05):\n",
    "              \n",
    "        \n",
    "        \"\"\"\n",
    "        Normality: Assumes that the error terms are normally distributed. If they are not,\n",
    "        nonlinear transformations of variables may solve this.\n",
    "\n",
    "        This assumption being violated primarily causes issues with the confidence intervals\n",
    "        \"\"\"\n",
    "    \n",
    "        print('Assumption 2: The error terms are normally distributed', '\\n')\n",
    "    \n",
    "    # Calculating residuals for the Anderson-Darling test\n",
    "        df_results = Visualizer.calculate_residuals(model, features, label)\n",
    "    \n",
    "        print('Using the Anderson-Darling test for normal distribution')\n",
    "\n",
    "    # Performing the test on the residuals\n",
    "        p_value = normal_ad(df_results['Residuals'])[1]\n",
    "        print('p-value from the test - below 0.05 generally means non-normal:', p_value)\n",
    "    \n",
    "    # Reporting the normality of the residuals\n",
    "        if p_value < p_value_thresh:\n",
    "            print('Residuals are not normally distributed')\n",
    "        else:\n",
    "            print('Residuals are normally distributed')\n",
    "    \n",
    "    # Plotting the residuals distribution\n",
    "        plt.subplots(figsize=(12, 6))\n",
    "        plt.title('Distribution of Residuals')\n",
    "        sns.distplot(df_results['Residuals'])\n",
    "        plt.show()\n",
    "    \n",
    "        print()\n",
    "        if p_value > p_value_thresh:\n",
    "            print('Assumption satisfied')\n",
    "        else:\n",
    "            print('Assumption not satisfied')\n",
    "            print()\n",
    "            print('Confidence intervals will likely be affected')\n",
    "            print('Try performing nonlinear transformations on variables')\n",
    "    \n",
    "    def linear_assumption(self,model, features, label):\n",
    "        \"\"\"\n",
    "        Linearity: Assumes that there is a linear relationship between the predictors and\n",
    "                   the response variable. If not, either a quadratic term or another\n",
    "                   algorithm should be used.\n",
    "        \"\"\"\n",
    "        print('Assumption 1: Linear Relationship between the Target and the Feature', '\\n')\n",
    "\n",
    "        print('Checking with a scatter plot of actual vs. predicted.',\n",
    "               'Predictions should follow the diagonal line.')\n",
    "\n",
    "        # Calculating residuals for the plot\n",
    "        df_results = Visualizer.calculate_residuals(model, features, label)\n",
    "\n",
    "        # Plotting the actual vs predicted values\n",
    "        sns.lmplot(x='Actual', y='Predicted', data=df_results, fit_reg=False, size=7)\n",
    "\n",
    "        # Plotting the diagonal line\n",
    "        line_coords = np.arange(df_results.min().min(), df_results.max().max())\n",
    "        plt.plot(line_coords, line_coords,  # X and y points\n",
    "                 color='darkorange', linestyle='--')\n",
    "        plt.title('Actual vs. Predicted')\n",
    "        plt.show()\n",
    "        \n",
    "    def homoscedasticity_assumption(self,model, features, label):\n",
    "        \"\"\"\n",
    "        Homoscedasticity: Assumes that the errors exhibit constant variance\n",
    "        \"\"\"\n",
    "        print('Assumption 5: Homoscedasticity of Error Terms', '\\n')\n",
    "\n",
    "        print('Residuals should have relative constant variance')\n",
    "\n",
    "        # Calculating residuals for the plot\n",
    "        df_results = Visualizer.calculate_residuals(model, features, label)\n",
    "\n",
    "        # Plotting the residuals\n",
    "        plt.subplots(figsize=(12, 6))\n",
    "        ax = plt.subplot(111)  # To remove spines\n",
    "        plt.scatter(x=df_results.index, y=df_results.Residuals, alpha=0.5)\n",
    "        plt.plot(np.repeat(0, df_results.index.max()), color='darkorange', linestyle='--')\n",
    "        ax.spines['right'].set_visible(False)  # Removing the right spine\n",
    "        ax.spines['top'].set_visible(False)  # Removing the top spine\n",
    "        plt.title('Residuals')\n",
    "        plt.show() \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def autocorrelation_assumption(self,model, features, label):\n",
    "        \"\"\"\n",
    "        Autocorrelation: Assumes that there is no autocorrelation in the residuals. If there is\n",
    "                         autocorrelation, then there is a pattern that is not explained due to\n",
    "                         the current value being dependent on the previous value.\n",
    "                         This may be resolved by adding a lag variable of either the dependent\n",
    "                         variable or some of the predictors.\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.stattools import durbin_watson\n",
    "        print('Assumption 4: No Autocorrelation', '\\n')\n",
    "\n",
    "        # Calculating residuals for the Durbin Watson-tests\n",
    "        df_results = Visualizer.calculate_residuals(model, features, label)\n",
    "\n",
    "        print('\\nPerforming Durbin-Watson Test')\n",
    "        print('Values of 1.5 < d < 2.5 generally show that there is no autocorrelation in the data')\n",
    "        print('0 to 2< is positive autocorrelation')\n",
    "        print('>2 to 4 is negative autocorrelation')\n",
    "        print('-------------------------------------')\n",
    "        durbinWatson = durbin_watson(df_results['Residuals'])\n",
    "        print('Durbin-Watson:', durbinWatson)\n",
    "        if durbinWatson < 1.5:\n",
    "            print('Signs of positive autocorrelation', '\\n')\n",
    "            print('Assumption not satisfied')\n",
    "        elif durbinWatson > 2.5:\n",
    "            print('Signs of negative autocorrelation', '\\n')\n",
    "            print('Assumption not satisfied')\n",
    "        else:\n",
    "            print('Little to no autocorrelation', '\\n')\n",
    "            print('Assumption satisfied')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enum, kayıp gözlem var mı , varsa hangi yöntemlerle doldurulmalı\n",
    "class Preprocess: \n",
    "\n",
    "    \n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        \n",
    "    def get_missing_values(self):\n",
    "        missing_values = self.data.isnull().sum()\n",
    "        missing_values.sort_values(ascending=False, inplace=True)\n",
    "        print(colored(\"------------  Missing Values  -------------\",'blue'))\n",
    "        return missing_values\n",
    "    \n",
    "    # column lardaki boş olan yerleri hangi metoda göre dolduracağımız: mean, medyan,mod ya da seçeceğimiz belirli bir değer\n",
    "    \n",
    "    def imputation(self,column,method):\n",
    "        \n",
    "        if method == \"median\":\n",
    "            self.data[column] = self.data[column].fillna(self.data[column].median())\n",
    "            \n",
    "        elif method == \"mode\":\n",
    "            self.data[column] = self.data[column].fillna(self.data[column].mode())\n",
    "            \n",
    "        elif method == \"mean\":\n",
    "            self.data[column] = self.data[column].fillna(self.data[column].mean())\n",
    "            \n",
    "        else:\n",
    "             self.data[column] = self.data[column].fillna(method)\n",
    "                \n",
    "        return self.get_missing_values()\n",
    "    \n",
    "    def drop(self,method):\n",
    "        \n",
    "        if method == \"any\":   #herhangi bir satırda NaN değeri varsa o satırı sil demek.\n",
    "            print(\"Drop Öncesi Data Shape -->   \",self.data.shape)\n",
    "            self.data.dropna(how=\"any\",inplace=True)\n",
    "            print(\"Drop Sonrası Data Shape -->   \",self.data.shape)\n",
    "            \n",
    "        elif method == \"all\":   #Tüm satır nan değerinde ise siler.\n",
    "            self.data.dropna(how=\"all\",inplace=True)\n",
    "        \n",
    "        # !!!!\n",
    "        \n",
    "        else:                  #sadece datanın method (method değerini data columnlarından birini yazarsak) oranındaki nan valuları siler.\n",
    "            self.data.dropna(subset=[method],inplace=True)\n",
    "            \n",
    "        \n",
    "        return self.get_missing_values()\n",
    "    \n",
    "    \n",
    "    def SMOTE(self, X, y):\n",
    "        \n",
    "        os = SMOTE(random_state=0)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "        columns = X_train.columns\n",
    "        os_data_X,os_data_y=os.fit_sample(X_train, y_train)\n",
    "        os_data_X = pd.DataFrame(data=os_data_X,columns=columns )\n",
    "        os_data_y= pd.DataFrame(data=os_data_y,columns=y_train.columns)\n",
    "        \n",
    "        return os_data_X, os_data_y\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridSearchHelper:\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Grid Search Helper...\")\n",
    "        index = [\"Logistic\", \"Decision Tree\",\"Random Forest\",\"Neural Network\",\"SVM\", \"BernoulliNB\", \"GaussianNB\",\"MultinomialNB\" , \"KNN\"]\n",
    "        columns = [\"Train\",\"Test\",\"Tuned\"]\n",
    "        self.Scores = pd.DataFrame(columns =columns, index = index)\n",
    "        \n",
    "        index_ = [\"Linear\", \"Decision Tree\",\"Random Forest\",\"Neural Network\",\"SVM\",\"KNN\"]\n",
    "        columns_ = [\"RMSE\",\"RMSE_MinMaxScaled\",\"RMSE_StandardScaled\", \"PCA\",\"Tuned\"]\n",
    "        self.RMSE = pd.DataFrame(columns =columns_, index = index_)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def AllScores(self):\n",
    "        return self.Scores\n",
    "    \n",
    "    def AllRMSE(self):\n",
    "        return self.RMSE\n",
    "    \n",
    "    \n",
    "    \n",
    "    def LogisticReg(self,X_train,X_test,y_train,y_test):\n",
    "        \n",
    "        print(colored('------------------     STATS  MODELS  --------------------\\n','blue'))\n",
    "        lj = sm.Logit(y_train,X_train)\n",
    "        ljm = lj.fit()\n",
    "        \n",
    "        print(ljm.summary())\n",
    "        \n",
    "        print()\n",
    "        print(colored('------------------     SCIKIT LEARN MODEL  --------------------\\n','blue'))\n",
    "        \n",
    "        #Scikit Learn Model\n",
    "        loj =LogisticRegression(solver = \"liblinear\")\n",
    "\n",
    "        loj_model = loj.fit(X_train,y_train)\n",
    "        train_accuracy = accuracy_score(y_train,loj_model.predict(X_train))\n",
    "        test_accuracy = accuracy_score(y_test,loj_model.predict(X_test))\n",
    "\n",
    "        print(colored(\"Intercept :  \",'red'),loj_model.intercept_)\n",
    "        print()\n",
    "        print(colored(\"Coefficient  : \",'red'),loj_model.coef_)\n",
    "        print()\n",
    "        \n",
    "        print(colored('TRAIN','red'))\n",
    "      \n",
    "        visualize = Visualizer()\n",
    "        \n",
    "        visualize.confusion_matris(y_train,loj_model.predict(X_train))\n",
    "        print(classification_report(y_train,loj_model.predict(X_train)))\n",
    "        print(colored(\"Train accuracy score : \",'blue'),train_accuracy)\n",
    "       \n",
    "        \n",
    "        print(colored('TEST','red'))\n",
    "      \n",
    "        visualize = Visualizer()\n",
    "        visualize.confusion_matris(y_test, loj_model.predict(X_test))\n",
    "        print(classification_report(y_test,loj_model.predict(X_test)))\n",
    "        print(colored(\"Test accuracy score : \",'blue'), test_accuracy)\n",
    "        \n",
    "        self.Scores[\"Train\"][\"Logistic\"] = train_accuracy\n",
    "        self.Scores[\"Test\"][\"Logistic\"] = test_accuracy\n",
    "    \n",
    "   \n",
    "        return loj_model  , loj_model.predict(X_test)\n",
    "    \n",
    "        \n",
    "           \n",
    "    \n",
    "    def DecisionTreeCls(self,X_train,X_test,y_train,y_test):\n",
    "           \n",
    "        # Model\n",
    "        \n",
    "        clf = DecisionTreeClassifier()\n",
    "        clf = clf.fit(X_train,y_train)\n",
    "        train_accuracy = accuracy_score(y_train,clf.predict(X_train))\n",
    "        test_accuracy = accuracy_score(y_test,clf.predict(X_test))\n",
    "      \n",
    "        print(colored('TRAIN','red'))\n",
    "      \n",
    "        visualize = Visualizer()\n",
    "        \n",
    "        visualize.confusion_matris(y_train, clf.predict(X_train))\n",
    "        print(classification_report(y_train,clf.predict(X_train)))\n",
    "        print(colored(\"Train accuracy score : \",'blue'),train_accuracy)\n",
    "       \n",
    "        \n",
    "        print(colored('TEST','red'))\n",
    "      \n",
    "       \n",
    "        visualize.confusion_matris(y_test, clf.predict(X_test))\n",
    "        print(classification_report(y_test,clf.predict(X_test)))\n",
    "        print(colored(\"Test accuracy score : \",'blue'), test_accuracy)\n",
    "        \n",
    "        self.Scores[\"Train\"][\"Decision Tree\"] = train_accuracy\n",
    "        self.Scores[\"Test\"][\"Decision Tree\"] = test_accuracy\n",
    "    \n",
    "   \n",
    "        return clf\n",
    "\n",
    "\n",
    "    def RandomForestCls(self,X_train,X_test,y_train,y_test):\n",
    "           \n",
    "        # Model\n",
    "        \n",
    "        clf = RandomForestClassifier()\n",
    "        clf = clf.fit(X_train,y_train)\n",
    "        train_accuracy = accuracy_score(y_train,clf.predict(X_train))\n",
    "        test_accuracy = accuracy_score(y_test,clf.predict(X_test))\n",
    "      \n",
    "        print(colored('TRAIN','red'))\n",
    "      \n",
    "        visualize = Visualizer()\n",
    "        visualize.confusion_matris(y_train, clf.predict(X_train))\n",
    "        print(classification_report(y_train,clf.predict(X_train)))\n",
    "        print(colored(\"Train accuracy score : \",'blue'),train_accuracy)\n",
    "       \n",
    "        \n",
    "        print(colored('TEST','red'))\n",
    "      \n",
    "        \n",
    "        visualize.confusion_matris(y_test, clf.predict(X_test))\n",
    "        print(classification_report(y_test,clf.predict(X_test)))\n",
    "        print(colored(\"Test accuracy score : \",'blue'), test_accuracy)\n",
    "        \n",
    "        self.Scores[\"Train\"][\"Random Forest\"] = train_accuracy\n",
    "        self.Scores[\"Test\"][\"Random Forest\"] = test_accuracy\n",
    "    \n",
    "   \n",
    "        return clf\n",
    "\n",
    "    def NeuralNetworkCls(self,X_train,X_test,y_train,y_test):\n",
    "           \n",
    "        # Model\n",
    "        \n",
    "        clf = MLPClassifier()\n",
    "        clf = clf.fit(X_train,y_train)\n",
    "        train_accuracy = accuracy_score(y_train,clf.predict(X_train))\n",
    "        test_accuracy = accuracy_score(y_test,clf.predict(X_test))\n",
    "      \n",
    "        print(colored('TRAIN','red'))\n",
    "      \n",
    "        visualize = Visualizer()\n",
    "        \n",
    "        visualize.confusion_matris(y_train, clf.predict(X_train))\n",
    "        print(classification_report(y_train,clf.predict(X_train)))\n",
    "        print(colored(\"Train accuracy score : \",'blue'),train_accuracy)\n",
    "       \n",
    "        \n",
    "        print(colored('TEST','red'))\n",
    "      \n",
    "        \n",
    "        visualize.confusion_matris(y_test, clf.predict(X_test))\n",
    "        print(classification_report(y_test,clf.predict(X_test)))\n",
    "        print(colored(\"Test accuracy score : \",'blue'), test_accuracy)\n",
    "        \n",
    "        self.Scores[\"Train\"][\"Neural Network\"] = train_accuracy\n",
    "        self.Scores[\"Test\"][\"Neural Network\"] = test_accuracy\n",
    "    \n",
    "   \n",
    "        return clf\n",
    "\n",
    "    def GaussianNBCls(self,X_train,X_test,y_train,y_test):\n",
    "           \n",
    "        # Model\n",
    "        \n",
    "        clf = GaussianNB()\n",
    "        clf = clf.fit(X_train,y_train)\n",
    "        train_accuracy = accuracy_score(y_train,clf.predict(X_train))\n",
    "        test_accuracy = accuracy_score(y_test,clf.predict(X_test))\n",
    "      \n",
    "        print(colored('TRAIN','red'))\n",
    "      \n",
    "        visualize = Visualizer()\n",
    "        \n",
    "        visualize.confusion_matris(y_train, clf.predict(X_train))\n",
    "        print(classification_report(y_train,clf.predict(X_train)))\n",
    "        print(colored(\"Train accuracy score : \",'blue'),train_accuracy)\n",
    "       \n",
    "        \n",
    "        print(colored('TEST','red'))\n",
    "      \n",
    "       \n",
    "        visualize.confusion_matris(y_test, clf.predict(X_test))\n",
    "        print(classification_report(y_test,clf.predict(X_test)))\n",
    "        print(colored(\"Test accuracy score : \",'blue'), test_accuracy)\n",
    "        \n",
    "        self.Scores[\"Train\"][\"GaussianNB\"] = train_accuracy\n",
    "        self.Scores[\"Test\"][\"GaussianNB\"] = test_accuracy\n",
    "    \n",
    "   \n",
    "        return clf\n",
    "\n",
    "    def BernoulliNBCls(self,X_train,X_test,y_train,y_test):\n",
    "           \n",
    "        # Model\n",
    "        \n",
    "        clf = BernoulliNB()\n",
    "        clf = clf.fit(X_train,y_train)\n",
    "        train_accuracy = accuracy_score(y_train,clf.predict(X_train))\n",
    "        test_accuracy = accuracy_score(y_test,clf.predict(X_test))\n",
    "      \n",
    "        print(colored('TRAIN','red'))\n",
    "      \n",
    "        visualize = Visualizer()\n",
    "        \n",
    "        visualize.confusion_matris(y_train, clf.predict(X_train))\n",
    "        print(classification_report(y_train,clf.predict(X_train)))\n",
    "        print(colored(\"Train accuracy score : \",'blue'),train_accuracy)\n",
    "       \n",
    "        \n",
    "        print(colored('TEST','red'))\n",
    "      \n",
    "        \n",
    "        visualize.confusion_matris(y_test, clf.predict(X_test))\n",
    "        print(classification_report(y_test,clf.predict(X_test)))\n",
    "        print(colored(\"Test accuracy score : \",'blue'), test_accuracy)\n",
    "        \n",
    "        self.Scores[\"Train\"][\"BernoulliNB\"] = train_accuracy\n",
    "        self.Scores[\"Test\"][\"BernoulliNB\"] = test_accuracy\n",
    "    \n",
    "   \n",
    "        return clf\n",
    "\n",
    "    def MultinomialNBCls(self,X_train,X_test,y_train,y_test):\n",
    "           \n",
    "        # Model\n",
    "        \n",
    "        clf = MultinomialNB()\n",
    "        clf = clf.fit(X_train,y_train)\n",
    "        train_accuracy = accuracy_score(y_train,clf.predict(X_train))\n",
    "        test_accuracy = accuracy_score(y_test,clf.predict(X_test))\n",
    "      \n",
    "        print(colored('TRAIN','red'))\n",
    "      \n",
    "        visualize = Visualizer()\n",
    "        \n",
    "        visualize.confusion_matris(y_train, clf.predict(X_train))\n",
    "        print(classification_report(y_train,clf.predict(X_train)))\n",
    "        print(colored(\"Train accuracy score : \",'blue'),train_accuracy)\n",
    "       \n",
    "        \n",
    "        print(colored('TEST','red'))\n",
    "      \n",
    "        \n",
    "        visualize.confusion_matris(y_test, clf.predict(X_test))\n",
    "        print(classification_report(y_test,clf.predict(X_test)))\n",
    "        print(colored(\"Test accuracy score : \",'blue'), test_accuracy)\n",
    "        \n",
    "        self.Scores[\"Train\"][\"MultinomialNB\"] = train_accuracy\n",
    "        self.Scores[\"Test\"][\"MultinomialNB\"] = test_accuracy\n",
    "    \n",
    "   \n",
    "        return clf\n",
    " \n",
    "\n",
    "    def SVMCls(self,X_train,X_test,y_train,y_test):\n",
    "           \n",
    "        # Model\n",
    "        \n",
    "        clf = SVC()\n",
    "        clf = clf.fit(X_train,y_train)\n",
    "        train_accuracy = accuracy_score(y_train,clf.predict(X_train))\n",
    "        test_accuracy = accuracy_score(y_test,clf.predict(X_test))\n",
    "      \n",
    "        print(colored('TRAIN','red'))\n",
    "      \n",
    "        visualize = Visualizer()\n",
    "        \n",
    "        visualize.confusion_matris(y_train, clf.predict(X_train))\n",
    "        print(classification_report(y_train,clf.predict(X_train)))\n",
    "        print(colored(\"Train accuracy score : \",'blue'),train_accuracy)\n",
    "       \n",
    "        \n",
    "        print(colored('TEST','red'))\n",
    "      \n",
    "        \n",
    "        visualize.confusion_matris(y_test, clf.predict(X_test))\n",
    "        print(classification_report(y_test,clf.predict(X_test)))\n",
    "        print(colored(\"Test accuracy score : \",'blue'), test_accuracy)\n",
    "        \n",
    "        self.Scores[\"Train\"][\"SVM\"] = train_accuracy\n",
    "        self.Scores[\"Test\"][\"SVM\"] = test_accuracy\n",
    "    \n",
    "   \n",
    "        return clf\n",
    "\n",
    "    def KNNCls(self,X_train,X_test,y_train,y_test):\n",
    "           \n",
    "        # Model\n",
    "        \n",
    "        clf =  KNeighborsClassifier()\n",
    "        clf = clf.fit(X_train,y_train)\n",
    "        train_accuracy = accuracy_score(y_train,clf.predict(X_train))\n",
    "        test_accuracy = accuracy_score(y_test,clf.predict(X_test))\n",
    "      \n",
    "        print(colored('TRAIN','red'))\n",
    "      \n",
    "        visualize = Visualizer()\n",
    "        \n",
    "        visualize.confusion_matris(y_train, clf.predict(X_train))\n",
    "        print(classification_report(y_train,clf.predict(X_train)))\n",
    "        print(colored(\"Train accuracy score : \",'blue'),train_accuracy)\n",
    "       \n",
    "        \n",
    "        print(colored('TEST','red'))\n",
    "      \n",
    "        \n",
    "        visualize.confusion_matris(y_test, clf.predict(X_test))\n",
    "        print(classification_report(y_test,clf.predict(X_test)))\n",
    "        print(colored(\"Test accuracy score : \",'blue'), test_accuracy)\n",
    "        \n",
    "        self.Scores[\"Train\"][\"KNN\"] = train_accuracy\n",
    "        self.Scores[\"Test\"][\"KNN\"] = test_accuracy\n",
    "    \n",
    "   \n",
    "        return clf\n",
    "\n",
    "# ----------------- Regression ---------------------------\n",
    "\n",
    "    def LinearReg(self,X_train,X_test,y_train,y_test):\n",
    "        \n",
    "        reg = LinearRegression()\n",
    "        reg.fit(X_train,y_train)\n",
    "        y_pred = reg.predict(X_test)\n",
    "        \n",
    "        print(\"RMSE : \", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        \n",
    "        \n",
    "        self.RMSE[\"RMSE\"][\"Linear\"] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        return reg\n",
    "        \n",
    "\n",
    "\n",
    "    def DecisionTreeReg(self,X_train,X_test,y_train,y_test):\n",
    "        \n",
    "        reg = DecisionTreeRegressor()\n",
    "        reg.fit(X_train,y_train)\n",
    "        y_pred = reg.predict(X_test)\n",
    "        \n",
    "        print(\"RMSE : \", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        \n",
    "        self.RMSE[\"RMSE\"][\"Decision Tree\"] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        return reg\n",
    "\n",
    "    def RandomForestReg(self,X_train,X_test,y_train,y_test):\n",
    "        \n",
    "        reg = RandomForestRegressor()\n",
    "        reg.fit(X_train,y_train)\n",
    "        y_pred = reg.predict(X_test)\n",
    "        \n",
    "        print(\"RMSE : \", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        \n",
    "        self.RMSE[\"RMSE\"][\"Random Forest\"] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        return reg\n",
    " \n",
    "    def NeuralNetworkReg(self,X_train,X_test,y_train,y_test):\n",
    "        \n",
    "        reg = MLPRegressor()\n",
    "        reg.fit(X_train,y_train)\n",
    "        y_pred = reg.predict(X_test)\n",
    "        \n",
    "        print(\"RMSE : \", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        \n",
    "        self.RMSE[\"RMSE\"][\"Neural Network\"] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        return reg\n",
    "    \n",
    "    def SVMReg(self,X_train,X_test,y_train,y_test):\n",
    "        \n",
    "        reg = SVR()\n",
    "        reg.fit(X_train,y_train)\n",
    "        y_pred = reg.predict(X_test)\n",
    "        \n",
    "        print(\"RMSE : \", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        \n",
    "        self.RMSE[\"RMSE\"][\"SVM\"] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        return reg\n",
    "\n",
    "    def KNNReg(self,X_train,X_test,y_train,y_test):\n",
    "        \n",
    "        reg = KNeighborsRegressor()\n",
    "        reg.fit(X_train,y_train)\n",
    "        y_pred = reg.predict(X_test)\n",
    "        \n",
    "        print(\"RMSE : \", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        \n",
    "        self.RMSE[\"RMSE\"][\"KNN\"] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        return reg\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    def GridSearchCVM(self,model_name,model,params,X_train,X_test,y_train,y_test):\n",
    "        grid = GridSearchCV(model, params, cv =10)\n",
    "        grid.fit(X_train, y_train)\n",
    "        print(\"En iyi parametresi :\"+ str(grid.best_params_))\n",
    "        best_params = grid.best_estimator_\n",
    "        predict = best_params.predict(X_test)\n",
    "        accuracy_tuned = accuracy_score(y_test,predict)\n",
    "        print(\"Tuned Model Accuracy Score : \",accuracy_tuned)\n",
    "        self.Scores[\"Tuned\"][model_name] = accuracy_tuned\n",
    "        \n",
    "        return best_params\n",
    "        \n",
    "    def RandomizedSearchCVM(self, model_name, model,params,X_train,X_test,y_train,y_test):\n",
    "        rv = RandomizedSearchCV(model,params,n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "        rv.fit(X_train, y_train)\n",
    "        print(\"En iyi parametresi :\"+ str(rv.best_params_))\n",
    "        best_params = rv.best_estimator_\n",
    "        predict = best_params.predict(X_test)\n",
    "        accuracy_tuned = accuracy_score(y_test,predict)\n",
    "        print(\"Tuned Model Accuracy Score : \",accuracy_tuned)\n",
    "        self.Scores[\"Tuned\"][model_name] = accuracy_tuned\n",
    "        \n",
    "        return best_params\n",
    "        \n",
    "\n",
    "    def MinMaxScaleModel(self,model_name,model,X_train,X_test,y_train,y_test):\n",
    "        \n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_s = scaler.transform(X_train)\n",
    "        X_test_s = scaler.transform(X_test)\n",
    "        \n",
    "        model.fit(X_train_s,y_train)\n",
    "        y_pred = model.predict(X_test_s)\n",
    "        \n",
    "        print(\"RMSE : \", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        \n",
    "        self.RMSE[\"RMSE_MinMaxScaled\"][model_name] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        return X_train_s, X_test_s\n",
    "        \n",
    "    \n",
    "    def StandardScaleModel(self,model_name,model,X_train,X_test,y_train,y_test):\n",
    "        \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_st = scaler.transform(X_train)\n",
    "        X_test_st = scaler.transform(X_test)\n",
    "        \n",
    "        model.fit(X_train_st,y_train)\n",
    "        y_pred = model.predict(X_test_st)\n",
    "        \n",
    "        print(\"RMSE : \", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        \n",
    "        self.RMSE[\"RMSE_StandardScaled\"][model_name] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        \n",
    "        return X_train_st, X_test_st\n",
    "        \n",
    "\n",
    "    def GridSearchCVR(self,model_name,model,params,X_train,X_test,y_train,y_test):\n",
    "        grid = GridSearchCV(model, params, cv =10)\n",
    "        grid.fit(X_train, y_train)\n",
    "        print(\"En iyi parametresi :\"+ str(grid.best_params_))\n",
    "        best_params = grid.best_estimator_\n",
    "        predict = best_params.predict(X_test)\n",
    "        rmse_tuned = np.sqrt(mean_squared_error(y_test, predict))\n",
    "        print(\"Tuned Model RMSE : \",rmse_tuned)\n",
    "        self.RMSE[\"Tuned\"][model_name] = rmse_tuned\n",
    "        \n",
    "        return best_params\n",
    "        \n",
    "    def RandomizedSearchCVR(self, model_name, model,params,X_train,X_test,y_train,y_test):\n",
    "        rv = RandomizedSearchCV(model,params,n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "        rv.fit(X_train, y_train)\n",
    "        print(\"En iyi parametresi :\"+ str(rv.best_params_))\n",
    "        best_params = rv.best_estimator_\n",
    "        predict = best_params.predict(X_test)\n",
    "        rmse_tuned = np.sqrt(mean_squared_error(y_test, predict))\n",
    "        print(\"Tuned Model RMSE : \",rmse_tuned)\n",
    "        self.RMSE[\"Tuned\"][model_name] = rmse_tuned       \n",
    "        \n",
    "        return best_params\n",
    "    \n",
    "\n",
    "    def PCAModel(self,model_name,model,X_train,X_test,y_train,y_test):\n",
    "        pca = PCA()\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "        model.fit(X_train_pca,y_train)\n",
    "        y_pred = model.predict(X_test_pca)\n",
    "        \n",
    "        self.RMSE[\"PCA\"][model_name] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "    \n",
    "    def pca_(self,X):\n",
    "        pca = PCA()\n",
    "        X_pca = pca.fit_transform(X)\n",
    "        return X_pca\n",
    "    \n",
    "    def minmaxscale(self,X):\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(X)\n",
    "        X_s = scaler.transform(X)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def Kmeans(self,data,n_cluster):\n",
    "        kmeans = KMeans(n_clusters = n_cluster)\n",
    "\n",
    "        clusters = kmeans.fit_predict(data)\n",
    "        return clusters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
